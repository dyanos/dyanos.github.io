---
layout: post
title: LDA사용하기
date: 2019-07-21
comments: true
external-url:
categories: NLP
---

> Gensim을 이용하여 LDA를 하는 코드를 적어놓는다. 

### Pandas에서 nan인 것들만 조건을 제거하기

```python
df[~df["Content"].isna()]
```

### LDA 코드

```python
import spacy
import re
from spacy.tokenizer import Tokenizer
from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex

sp = spacy.load("en_core_web_lg")

def custom_tokenizer(nlp):
    infix_re = re.compile(r'''[.\,\?\:\;\...\‘\’\`\“\”\"\'~]''')
    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)
    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)

    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,
                                suffix_search=suffix_re.search,
                                infix_finditer=infix_re.finditer,
                                token_match=None)

# 'non-native'와 같은 단어들도 tokenizing할 수 있도록 수정
sp.tokenizer = custom_tokenizer(sp)

# stopword를 nltk에서 불러옴
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stopWords = set(stopwords.words('english'))

#https://stackoverflow.com/questions/46148182/issues-in-getting-trigrams-using-gensim
# 위 tokenizer를 이용하여 bigram, trigram, fourgram계산 
# To remove stopwords
sentence_stream = ndf['Content'].apply(lambda x: [y.text.lower() for y in sp(x.strip()) if y.pos_ != 'PUNCT' and y.text.lower() not in stopWords]).values.tolist()
bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')
trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')
fourgram = Phrases(trigram[sentence_stream], min_count=1, delimiter=b' ')

bigram_stack, trigram_stack, fourgram_stack = [], [], []
for sent in sentence_stream:
    bigrams_ = [b for b in bigram[sent] if b.count(' ') == 1]
    trigrams_ = [t for t in trigram[bigram[sent]] if t.count(' ') == 2]
    fourgrams_ = [t for t in fourgram[trigram[bigram[sent]]] if t.count(' ') == 3]
    bigram_stack.append(bigrams_)
    trigram_stack.append(trigrams_)
    fourgram_stack.append(fourgrams_)

import gensim
import gensim.corpora as corpora

# Create Dictionary
id2word = corpora.Dictionary(bigram_stack)

# Create Corpus
texts = bigram_stack

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
    id2word=id2word,
    num_topics=20,
    random_state=100,
    update_every=1,
    chunksize=100,
    passes=10,
    alpha='auto',
    per_word_topics=True)

# 각 topic group별 keyword들을 table로 표시!!
#http://blog.naver.com/PostView.nhn?blogId=upennsolution&logNo=221437143732&from=search&redirect=Log&widgetTypeCall=true&directAccess=false
NUM_TOPICS = 20

word_dict = {};

for i in range(NUM_TOPICS):
    words = lda_model.show_topic(i, topn = 20)
    word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]

pd.DataFrame(word_dict)

# Visualize the topics
# !pip install pyLDAvis -> conda를 쓰는 경우는 conda로 설치하는 것을 추천
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)
vis
```

### TfIdf하는 방법은[Source]()

앞에서 수행했던 코드에서 
```python
from gensim.models import TfidfModel
tfidf = TfidfModel(corpus)

# pip install wordcloud
from wordcloud import WordCloud

def DrawWordCloudBy(year: int):
    sentence_stream = ndf.query('Year == {}'.format(year))['Content'].apply(lambda x: [y.text.lower() for y in sp(x.strip()) if y.pos_ != 'PUNCT' and y.text.lower() not in stopWords]).values.tolist()
    bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')

    bigram_stack = []
    for sent in sentence_stream:
        bigrams_ = [b for b in bigram[sent] if b.count(' ') == 1]
        bigram_stack.append(bigrams_)

    # Create Dictionary
    id2word = corpora.Dictionary(bigram_stack)

    # Create Corpus
    texts = bigram_stack

    # Term Document Frequency
    corpus = [id2word.doc2bow(text) for text in texts]

    tfidf = TfidfModel(corpus)

    weights = tfidf[flatten(corpus)]

    # Get terms from the dictionary and pair with weights

    weights = [(id2word[pair[0]], pair[1]) for pair in weights]

    # Initialize the word cloud

    wc = WordCloud(
        background_color="white",
        max_words=40,
        width = 800,
        height = 600,
        stopwords=stopwords.words("english")
    )

    wc.generate_from_frequencies(dict(weights))
    return wc

DrawWordCloudBy(2019).to_image()
```
